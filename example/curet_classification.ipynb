{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "# def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     for inputs, targets in loader:\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item() * inputs.size(0)\n",
    "#         correct += (outputs.argmax(dim=1) == targets).sum().item()\n",
    "\n",
    "#     avg_loss = total_loss / len(loader.dataset)\n",
    "#     accuracy = correct / len(loader.dataset)\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# def evaluate(model, loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in loader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             total_loss += loss.item() * inputs.size(0)\n",
    "#             correct += (outputs.argmax(dim=1) == targets).sum().item()\n",
    "#     avg_loss = total_loss / len(loader.dataset)\n",
    "#     accuracy = correct / len(loader.dataset)\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# class CuretDataset(Dataset):\n",
    "#     def __init__(self, root_dir, transform=None):\n",
    "#         self.root_dir = Path(root_dir)\n",
    "#         self.transform = transform\n",
    "#         self.image_paths = []\n",
    "#         self.labels = []\n",
    "#         self.classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "#         self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "\n",
    "#         for cls in self.classes:\n",
    "#             cls_folder = self.root_dir / cls\n",
    "#             for img_file in sorted(cls_folder.glob(\"*.png\")) + sorted(cls_folder.glob(\"*.jpg\")) + sorted(cls_folder.glob(\"*.bmp\")):\n",
    "#                 self.image_paths.append(img_file)\n",
    "#                 self.labels.append(self.class_to_idx[cls])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = self.image_paths[idx]\n",
    "#         image = Image.open(img_path).convert(\"L\")  # grayscale\n",
    "#         label = self.labels[idx]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image, label\n",
    "\n",
    "\n",
    "\n",
    "# def get_curet_loaders(batch_size=64):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#     ])\n",
    "#     dataset = CuretDataset(\"./data/curetgrey\", transform=transform)\n",
    "#     train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 46, 46], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     return train_loader, test_loader\n",
    "\n",
    "def run_curet_experiment(scale_J, angle_K, kernel_size, model, lr, image_shape=(1, 200), nb_class=61):\n",
    "    \n",
    "    train_loader, test_loader = get_curet_loaders(batch_size=64)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(6):  # Change number of epochs as needed\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f}, Test Acc={test_acc:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: J=3, K=4, kernel=9\n",
      "Creating filter bank with Sampling support width=8, Size=9, Angles=4 ...\n",
      "Creating filter bank with Sampling support width=8, Size=17, Angles=4 ...\n",
      "Creating filter bank with Sampling support width=8, Size=33, Angles=4 ...\n",
      "Epoch 1: Train Acc=0.2787, Test Acc=0.3043\n",
      "Epoch 2: Train Acc=0.6286, Test Acc=0.6304\n",
      "Epoch 3: Train Acc=0.7693, Test Acc=0.6522\n",
      "Epoch 4: Train Acc=0.8611, Test Acc=0.7826\n",
      "Epoch 5: Train Acc=0.8999, Test Acc=0.8261\n",
      "Epoch 6: Train Acc=0.9170, Test Acc=0.8043\n",
      "Epoch 1: Train Acc=0.9319, Test Acc=0.8261\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model.linear.parameters():\n\u001b[32m     33\u001b[39m     param.requires_grad = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m models.append(\u001b[43mrun_curet_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mrun_curet_experiment\u001b[39m\u001b[34m(scale_J, angle_K, kernel_size, model, lr, image_shape, nb_class)\u001b[39m\n\u001b[32m     86\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m6\u001b[39m):  \u001b[38;5;66;03m# Change number of epochs as needed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m     16\u001b[39m     loss.backward()\n\u001b[32m     17\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * inputs.size(\u001b[32m0\u001b[39m)\n\u001b[32m     20\u001b[39m     correct += (outputs.argmax(dim=\u001b[32m1\u001b[39m) == targets).sum().item()\n\u001b[32m     22\u001b[39m avg_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(loader.dataset)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to ../src and append it to sys.path\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "sys.path.append(src_path)\n",
    "from dense import dense\n",
    "import torch\n",
    "from torch import nn\n",
    "# Try different combinations\n",
    "models = []\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "J_params = [3]\n",
    "K_params = [4]\n",
    "kernel_size_params = [9]\n",
    "for J in J_params:\n",
    "    for K in K_params:\n",
    "        for ks in kernel_size_params:\n",
    "            print(f\"\\nRunning: J={J}, K={K}, kernel={ks}\")\n",
    "            model = dense(J, K, (1, 200), ks, 61).to(device)\n",
    "            for conv in model.sequential_conv:\n",
    "                for param in conv.parameters():\n",
    "                    param.requires_grad = False\n",
    "            for param in model.linear.parameters():\n",
    "                param.requires_grad = True\n",
    "            models.append(run_curet_experiment(J, K, ks, model, 1e-3))\n",
    "\n",
    "for model in models:\n",
    "    for conv in model.sequential_conv:\n",
    "        for param in conv.parameters():\n",
    "            param.requires_grad = True\n",
    "    for param in model.linear.parameters():\n",
    "        param.requires_grad = False\n",
    "    models.append(run_curet_experiment(J, K, ks, model, 1e-5))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "# Needs to run \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def extract_features_and_labels(model, dataloader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            feats = model(images).view(images.size(0), -1).cpu().numpy()\n",
    "            features.append(feats)\n",
    "            labels.extend(targets.numpy())\n",
    "    return np.vstack(features), np.array(labels)\n",
    "\n",
    "\n",
    "# Try different combinations\n",
    "models = []\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "J_params = [2]\n",
    "K_params = [4]\n",
    "kernel_size_params = [9]\n",
    "for J in J_params:\n",
    "for K in K_params:\n",
    "    for ks in kernel_size_params:\n",
    "        print(f\"\\nRunning: J={J}, K={K}, kernel={ks}\")\n",
    "        model = dense(J, K, (1, 200), ks).to(device)\n",
    "        train_loader, test_loader = get_curet_loaders(batch_size=64)\n",
    "        train_feats, train_labels = extract_features_and_labels(model, train_loader)\n",
    "        test_feats, test_labels = extract_features_and_labels(model, test_loader)\n",
    "\n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=0.95)  # or fixed value like 100\n",
    "        train_pca = pca.fit_transform(train_feats)\n",
    "        test_pca = pca.transform(test_feats)\n",
    "\n",
    "        # Train classifier (logistic regression or nearest centroid, etc.)\n",
    "        clf = LogisticRegression(max_iter=1000)\n",
    "        clf.fit(train_pca, train_labels)\n",
    "\n",
    "        # Evaluate\n",
    "        preds = clf.predict(test_pca)\n",
    "        acc = accuracy_score(test_labels, preds)\n",
    "        print(f\"âœ… PCA classifier test accuracy: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scatter_net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
