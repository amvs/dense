# model setting
wavelet: meyer
max_scale: 2
nb_orients: 4
share_channels: false
n_copies: 1
depth: -1  # -1 means full scatter
random: false
use_original_filters: false  # If true, use filters at different scales without downsampling. If false, use first filter and downsample (current mode).
use_log_transform: false  # If true, apply log(1+x) transform to scattering coefficients before classification

# classifier setting
classifier_type: hypernetwork
hypernet_hidden_dim: 64
attention_d_model: 128
attention_num_heads: 4
attention_num_layers: 2
pca_dim: 50  # Number of PCA components per class (used for 'pca', 'trainable_pca', 'lowrank_pca')
lowrank_pca_rank: 100  # Rank of shared basis for low-rank PCA (should be << feature_dim, typically 50-200)

# training setting
dataset: kthtips
kth_root_dir: data/datasets/KTH_TIPS  # Path to KTH-TIPS (original) dataset root directory
resize: 200
classifier_epochs: 1
conv_epochs: 1
batch_size: 30  # Should be multiple of 10 (num_classes) when use_balanced_batches=true
lr: 0.05
linear_lr: 0.01
weight_decays: 1e-4
lambda_reg: 1.0e-4
fine_tune_mode: extractor_only  # Options: 'extractor_only' (freeze classifier, fine-tune extractor) or 'both' (fine-tune both together)
ft_lr_multiplier: 0.1  # Learning rate multiplier for fine-tuning (applied to both extractor and classifier)
max_grad_norm: 1.0  # Maximum gradient norm for clipping
# Data size control (NEW: use example_per_class instead of train_ratio)
example_per_class: null  # Number of examples per class for training (null = use all available)
test_ratio: 0.15  # Test set ratio (train:val:test = 0.7:0.15:0.15 by default)
train_val_ratio: 4  # Not used for KTH-TIPS (uses standard split)
use_balanced_batches: true  # Ensure each batch has equal examples per class
use_scale_augmentation: false  # If true, augment training images with scale factors [1, sqrt(2), 2, 2*sqrt(2)]
seed: 123

# dataset note
# classes=10, each class has 81 images
